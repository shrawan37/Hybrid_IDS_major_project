{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9aa0e31c",
   "metadata": {},
   "source": [
    "# Frontend Integration Guide\n",
    "## How to Use the Trained Model with Frontend Data\n",
    "\n",
    "This notebook demonstrates how to load the trained model and preprocessing artifacts, then use them to make predictions on new data from the frontend.\n",
    "\n",
    "**Output Format:**\n",
    "- **0 = NORMAL** (No attack detected - safe traffic)\n",
    "- **1 = ATTACK** (Anomaly detected - suspicious traffic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "094b06dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"‚úÖ Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a669def",
   "metadata": {},
   "source": [
    "## Step 1: Load All Saved Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa090db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved artifacts...\n",
      "\n",
      "‚úÖ Model loaded: isolation_forest_frontend.pkl\n",
      "‚úÖ Features loaded: 17 features\n",
      "‚úÖ Scaler loaded: scaler_frontend.pkl\n",
      "‚úÖ Encoder loaded: encoder_frontend.pkl\n",
      "‚úÖ Frequency encoding loaded: freq_encoding_frontend.pkl\n",
      "\n",
      "üìã Frontend Features Used:\n",
      "    1. srv_count\n",
      "    2. service\n",
      "    3. dst_host_srv_count\n",
      "    4. dst_host_same_srv_rate\n",
      "    5. count\n",
      "    6. dst_host_count\n",
      "    7. rerror_rate\n",
      "    8. logged_in\n",
      "    9. flag_SF\n",
      "   10. srv_rerror_rate\n",
      "   11. protocol_type_tcp\n",
      "   12. dst_host_srv_rerror_rate\n",
      "   13. dst_host_rerror_rate\n",
      "   14. src_bytes\n",
      "   15. dst_bytes\n",
      "   16. dst_host_same_src_port_rate\n",
      "   17. protocol_type_udp\n"
     ]
    }
   ],
   "source": [
    "# Load all saved model and preprocessing artifacts\n",
    "print(\"Loading saved artifacts...\\n\")\n",
    "\n",
    "# Load the trained model\n",
    "model = joblib.load('isolation_forest_frontend.pkl')\n",
    "print(\"‚úÖ Model loaded: isolation_forest_frontend.pkl\")\n",
    "\n",
    "# Load feature list\n",
    "frontend_features = joblib.load('frontend_features.pkl')\n",
    "print(f\"‚úÖ Features loaded: {len(frontend_features)} features\")\n",
    "\n",
    "# Load preprocessors\n",
    "scaler = joblib.load('scaler_frontend.pkl')\n",
    "print(\"‚úÖ Scaler loaded: scaler_frontend.pkl\")\n",
    "\n",
    "encoder = joblib.load('encoder_frontend.pkl')\n",
    "print(\"‚úÖ Encoder loaded: encoder_frontend.pkl\")\n",
    "\n",
    "freq_encoding = joblib.load('freq_encoding_frontend.pkl')\n",
    "print(\"‚úÖ Frequency encoding loaded: freq_encoding_frontend.pkl\")\n",
    "\n",
    "print(f\"\\nüìã Frontend Features Used:\")\n",
    "for i, feat in enumerate(frontend_features, 1):\n",
    "    print(f\"   {i:2d}. {feat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c970a5",
   "metadata": {},
   "source": [
    "## Step 2: Create Dummy Data (Simulating Frontend Input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03895b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üìã DUMMY DATA - 17 PRE-PROCESSED FRONTEND FEATURES\n",
      "======================================================================\n",
      "\n",
      "‚úÖ Created 5 dummy records with 17 pre-processed features\n",
      "\n",
      "Dataset shape: (5, 17)\n",
      "\n",
      "Feature names (17 features):\n",
      "    1. srv_count\n",
      "    2. service\n",
      "    3. dst_host_srv_count\n",
      "    4. dst_host_same_srv_rate\n",
      "    5. count\n",
      "    6. dst_host_count\n",
      "    7. rerror_rate\n",
      "    8. logged_in\n",
      "    9. flag_SF\n",
      "   10. srv_rerror_rate\n",
      "   11. protocol_type_tcp\n",
      "   12. dst_host_srv_rerror_rate\n",
      "   13. dst_host_rerror_rate\n",
      "   14. src_bytes\n",
      "   15. dst_bytes\n",
      "   16. dst_host_same_src_port_rate\n",
      "   17. protocol_type_udp\n",
      "\n",
      "Dummy Data Sample (first record):\n",
      "srv_count                        8.00\n",
      "service                          0.08\n",
      "dst_host_srv_count             255.00\n",
      "dst_host_same_srv_rate           1.00\n",
      "count                            8.00\n",
      "dst_host_count                 255.00\n",
      "rerror_rate                      0.00\n",
      "logged_in                        1.00\n",
      "flag_SF                          1.00\n",
      "srv_rerror_rate                  0.00\n",
      "protocol_type_tcp                1.00\n",
      "dst_host_srv_rerror_rate         0.00\n",
      "dst_host_rerror_rate             0.00\n",
      "src_bytes                       -0.50\n",
      "dst_bytes                       -0.30\n",
      "dst_host_same_src_port_rate      0.00\n",
      "protocol_type_udp                0.00\n",
      "Name: 0, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Create 5 dummy records with ONLY the 17 frontend features\n",
    "# These are ALREADY PREPROCESSED features that your frontend sends\n",
    "# NO preprocessing needed - frontend already sends processed data\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üìã DUMMY DATA - 17 PRE-PROCESSED FRONTEND FEATURES\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "dummy_data = {\n",
    "    'srv_count': [8, 1, 5, 2, 200],\n",
    "    'service': [0.08, 0.15, 0.08, 0.02, 0.15],  # Already frequency encoded\n",
    "    'dst_host_srv_count': [255, 255, 255, 1, 5],\n",
    "    'dst_host_same_srv_rate': [1.0, 1.0, 1.0, 1.0, 0.05],\n",
    "    'count': [8, 10, 5, 2, 100],\n",
    "    'dst_host_count': [255, 255, 255, 1, 10],\n",
    "    'rerror_rate': [0.0, 0.0, 0.0, 0.0, 0.5],\n",
    "    'logged_in': [1, 0, 1, 0, 0],\n",
    "    'flag_SF': [1, 1, 1, 1, 0],  # Already one-hot encoded\n",
    "    'srv_rerror_rate': [0.0, 0.0, 0.0, 0.0, 0.5],\n",
    "    'protocol_type_tcp': [1, 1, 0, 0, 1],  # Already one-hot encoded\n",
    "    'dst_host_srv_rerror_rate': [0.0, 0.0, 0.0, 0.0, 0.3],\n",
    "    'dst_host_rerror_rate': [0.0, 0.0, 0.0, 0.0, 0.3],\n",
    "    'src_bytes': [-0.5, 0.2, 0.1, -0.8, 0.5],  # Already scaled\n",
    "    'dst_bytes': [-0.3, 0.8, 0.4, -0.9, 1.5],  # Already scaled\n",
    "    'dst_host_same_src_port_rate': [0.0, 0.0, 0.0, 0.0, 1.0],\n",
    "    'protocol_type_udp': [0, 0, 1, 0, 0]  # Already one-hot encoded\n",
    "}\n",
    "\n",
    "df_raw = pd.DataFrame(dummy_data)\n",
    "print(f\"‚úÖ Created {len(df_raw)} dummy records with 17 pre-processed features\")\n",
    "print(f\"\\nDataset shape: {df_raw.shape}\")\n",
    "print(f\"\\nFeature names ({len(df_raw.columns)} features):\")\n",
    "for i, col in enumerate(df_raw.columns, 1):\n",
    "    print(f\"   {i:2d}. {col}\")\n",
    "print(f\"\\nDummy Data Sample (first record):\")\n",
    "print(df_raw.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30fa04e",
   "metadata": {},
   "source": [
    "## Step 3: Preprocess Frontend Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e161ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üìù DATA STATUS\n",
      "======================================================================\n",
      "\n",
      "‚úÖ Data is ALREADY preprocessed:\n",
      "   - Numeric features: Already scaled (mean=0, std=1)\n",
      "   - Categorical features: Already one-hot encoded (flag_SF, protocol_type_tcp, protocol_type_udp)\n",
      "   - Ordinal features: Already frequency encoded (service)\n",
      "\n",
      "‚úÖ Ready for prediction!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ‚≠ê IMPORTANT: Frontend sends 17 ALREADY-PROCESSED features\n",
    "# No preprocessing needed! Data is ready for model prediction\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üìù DATA STATUS\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n‚úÖ Data is ALREADY preprocessed:\")\n",
    "print(\"   - Numeric features: Already scaled (mean=0, std=1)\")\n",
    "print(\"   - Categorical features: Already one-hot encoded (flag_SF, protocol_type_tcp, protocol_type_udp)\")\n",
    "print(\"   - Ordinal features: Already frequency encoded (service)\")\n",
    "print(\"\\n‚úÖ Ready for prediction!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edcf827",
   "metadata": {},
   "source": [
    "## Step 4: Select Only Frontend Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38b2a001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "‚úÖ DATA READY FOR PREDICTION\n",
      "======================================================================\n",
      "\n",
      "Feature matrix shape: (5, 17)\n",
      "Expected features: 17\n",
      "Actual features: 17\n",
      "\n",
      "‚úÖ All 17 features present and verified!\n"
     ]
    }
   ],
   "source": [
    "# Data is already preprocessed with exactly 17 features\n",
    "df_processed = df_raw.copy()\n",
    "\n",
    "# Select the 17 features for prediction\n",
    "X_frontend = df_processed[frontend_features]\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"‚úÖ DATA READY FOR PREDICTION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nFeature matrix shape: {X_frontend.shape}\")\n",
    "print(f\"Expected features: {len(frontend_features)}\")\n",
    "print(f\"Actual features: {X_frontend.shape[1]}\")\n",
    "\n",
    "if X_frontend.shape[1] == len(frontend_features):\n",
    "    print(\"\\n‚úÖ All 17 features present and verified!\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Warning: Feature count mismatch!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02a2442",
   "metadata": {},
   "source": [
    "## Step 5: Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09f1c3d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üîç PREDICTION RESULTS\n",
      "======================================================================\n",
      "\n",
      "üìå OUTPUT MAPPING:\n",
      "   0 = NORMAL   ‚úÖ (No attack - safe traffic)\n",
      "   1 = ATTACK   ‚ö†Ô∏è  (Anomaly detected - suspicious traffic)\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Record     Prediction      Interpretation                \n",
      "----------------------------------------------------------------------\n",
      "Record 1   1               ‚ö†Ô∏è  ATTACK - Suspicious       \n",
      "Record 2   1               ‚ö†Ô∏è  ATTACK - Suspicious       \n",
      "Record 3   1               ‚ö†Ô∏è  ATTACK - Suspicious       \n",
      "Record 4   1               ‚ö†Ô∏è  ATTACK - Suspicious       \n",
      "Record 5   1               ‚ö†Ô∏è  ATTACK - Suspicious       \n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Make predictions using the trained model\n",
    "raw_predictions = model.predict(X_frontend)\n",
    "\n",
    "# Convert from model format (-1 for outlier, 1 for inlier) to binary (0=normal, 1=attack)\n",
    "predictions = np.where(raw_predictions == -1, 1, 0)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üîç PREDICTION RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nüìå OUTPUT MAPPING:\")\n",
    "print(f\"   0 = NORMAL   ‚úÖ (No attack - safe traffic)\")\n",
    "print(f\"   1 = ATTACK   ‚ö†Ô∏è  (Anomaly detected - suspicious traffic)\")\n",
    "print(f\"\\n\" + \"-\"*70)\n",
    "print(f\"{'Record':<10} {'Prediction':<15} {'Interpretation':<30}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for i, pred in enumerate(predictions):\n",
    "    if pred == 0:\n",
    "        interpretation = \"‚úÖ NORMAL - Safe\"\n",
    "    else:\n",
    "        interpretation = \"‚ö†Ô∏è  ATTACK - Suspicious\"\n",
    "    print(f\"Record {i+1:<3} {pred:<15} {interpretation:<30}\")\n",
    "\n",
    "print(\"-\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3040d2a4",
   "metadata": {},
   "source": [
    "## Step 6: Create Production Response Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06e32523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì§ RESPONSE FORMAT FOR FRONTEND:\n",
      "\n",
      "{\n",
      "  \"status\": \"success\",\n",
      "  \"model\": \"IsolationForest\",\n",
      "  \"model_type\": \"Anomaly Detection\",\n",
      "  \"total_records\": 5,\n",
      "  \"total_attacks_detected\": 5,\n",
      "  \"total_normal\": 0,\n",
      "  \"predictions\": [\n",
      "    {\n",
      "      \"record_id\": 1,\n",
      "      \"prediction\": 1,\n",
      "      \"label\": \"ATTACK\",\n",
      "      \"risk_level\": \"HIGH\",\n",
      "      \"timestamp\": \"2026-02-02T14:57:44.671783\"\n",
      "    },\n",
      "    {\n",
      "      \"record_id\": 2,\n",
      "      \"prediction\": 1,\n",
      "      \"label\": \"ATTACK\",\n",
      "      \"risk_level\": \"HIGH\",\n",
      "      \"timestamp\": \"2026-02-02T14:57:44.671783\"\n",
      "    },\n",
      "    {\n",
      "      \"record_id\": 3,\n",
      "      \"prediction\": 1,\n",
      "      \"label\": \"ATTACK\",\n",
      "      \"risk_level\": \"HIGH\",\n",
      "      \"timestamp\": \"2026-02-02T14:57:44.671783\"\n",
      "    },\n",
      "    {\n",
      "      \"record_id\": 4,\n",
      "      \"prediction\": 1,\n",
      "      \"label\": \"ATTACK\",\n",
      "      \"risk_level\": \"HIGH\",\n",
      "      \"timestamp\": \"2026-02-02T14:57:44.671783\"\n",
      "    },\n",
      "    {\n",
      "      \"record_id\": 5,\n",
      "      \"prediction\": 1,\n",
      "      \"label\": \"ATTACK\",\n",
      "      \"risk_level\": \"HIGH\",\n",
      "      \"timestamp\": \"2026-02-02T14:57:44.671783\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Create response in JSON format for frontend\n",
    "print(\"\\nüì§ RESPONSE FORMAT FOR FRONTEND:\\n\")\n",
    "\n",
    "# Create detailed predictions with confidence and labels\n",
    "results = []\n",
    "for i, pred in enumerate(predictions):\n",
    "    result = {\n",
    "        \"record_id\": i + 1,\n",
    "        \"prediction\": int(pred),\n",
    "        \"label\": \"ATTACK\" if pred == 1 else \"NORMAL\",\n",
    "        \"risk_level\": \"HIGH\" if pred == 1 else \"LOW\",\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "    results.append(result)\n",
    "\n",
    "# Create final response\n",
    "response = {\n",
    "    \"status\": \"success\",\n",
    "    \"model\": \"IsolationForest\",\n",
    "    \"model_type\": \"Anomaly Detection\",\n",
    "    \"total_records\": len(predictions),\n",
    "    \"total_attacks_detected\": int(np.sum(predictions)),\n",
    "    \"total_normal\": int(len(predictions) - np.sum(predictions)),\n",
    "    \"predictions\": results\n",
    "}\n",
    "\n",
    "print(json.dumps(response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ca1878",
   "metadata": {},
   "source": [
    "## Step 7: Complete Workflow Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4cceb19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üß™ TESTING THE PRODUCTION FUNCTION\n",
      "======================================================================\n",
      "\n",
      "Single Record Prediction:\n",
      "{\n",
      "  \"predictions\": [\n",
      "    0\n",
      "  ],\n",
      "  \"labels\": [\n",
      "    \"NORMAL\"\n",
      "  ],\n",
      "  \"is_attack\": false\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PRESTIGE\\AppData\\Local\\Temp\\ipykernel_23908\\673364787.py:41: ChainedAssignmentError: A value is being set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "Such inplace method never works to update the original DataFrame or Series, because the intermediate object on which we are setting values always behaves as a copy (due to Copy-on-Write).\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' instead, to perform the operation inplace on the original object, or try to avoid an inplace operation using 'df[col] = df[col].method(value)'.\n",
      "\n",
      "See the documentation for a more detailed explanation: https://pandas.pydata.org/pandas-docs/stable/user_guide/copy_on_write.html\n",
      "  df['service'].fillna(freq_encoding.median(), inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PRODUCTION FUNCTION - Copy this to your backend/API\n",
    "# ============================================================\n",
    "\n",
    "def predict_network_anomaly(data_dict):\n",
    "    \"\"\"\n",
    "    Predict if network traffic is normal or anomalous.\n",
    "    \n",
    "    Args:\n",
    "        data_dict (dict): Dictionary containing the 17 required features\n",
    "                         Can be a single record or multiple records\n",
    "    \n",
    "    Returns:\n",
    "        dict: Predictions with labels (0=NORMAL, 1=ATTACK)\n",
    "    \"\"\"\n",
    "    # Convert to DataFrame\n",
    "    if isinstance(data_dict, dict) and not isinstance(list(data_dict.values())[0], list):\n",
    "        # Single record\n",
    "        df = pd.DataFrame([data_dict])\n",
    "    else:\n",
    "        # Multiple records\n",
    "        df = pd.DataFrame(data_dict)\n",
    "    \n",
    "    # Preprocess\n",
    "    numeric_features = ['duration', 'src_bytes', 'dst_bytes', 'land', 'wrong_fragment',\n",
    "                        'urgent', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised',\n",
    "                        'root_shell', 'su_attempted', 'num_root', 'num_file_creations',\n",
    "                        'num_shells', 'num_access_files', 'num_outbound_cmds', 'is_host_login',\n",
    "                        'is_guest_login', 'count', 'srv_count', 'dst_host_count',\n",
    "                        'dst_host_srv_count', 'level']\n",
    "    \n",
    "    df[numeric_features] = scaler.transform(df[numeric_features])\n",
    "    \n",
    "    categorical_features = ['flag', 'protocol_type']\n",
    "    encoded_features = encoder.transform(df[categorical_features])\n",
    "    encoded_df = pd.DataFrame(encoded_features, columns=encoder.get_feature_names_out(categorical_features))\n",
    "    df = df.drop(columns=categorical_features)\n",
    "    df = pd.concat([df, encoded_df], axis=1)\n",
    "    \n",
    "    df['service'] = df['service'].map(freq_encoding)\n",
    "    df['service'].fillna(freq_encoding.median(), inplace=True)\n",
    "    \n",
    "    # Select features\n",
    "    X = df[frontend_features]\n",
    "    \n",
    "    # Predict\n",
    "    raw_preds = model.predict(X)\n",
    "    predictions = np.where(raw_preds == -1, 1, 0)\n",
    "    \n",
    "    # Format response\n",
    "    return {\n",
    "        \"predictions\": predictions.tolist(),\n",
    "        \"labels\": [\"ATTACK\" if p == 1 else \"NORMAL\" for p in predictions],\n",
    "        \"is_attack\": bool(any(predictions == 1))\n",
    "    }\n",
    "\n",
    "\n",
    "# Test the function\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üß™ TESTING THE PRODUCTION FUNCTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Test with single record\n",
    "single_record = {\n",
    "    'duration': 100, 'protocol_type': 'tcp', 'service': 'http', 'flag': 'SF',\n",
    "    'src_bytes': 200, 'dst_bytes': 1000, 'land': 0, 'wrong_fragment': 0,\n",
    "    'urgent': 0, 'hot': 0, 'num_failed_logins': 0, 'logged_in': 1,\n",
    "    'num_compromised': 0, 'root_shell': 0, 'su_attempted': 0, 'num_root': 0,\n",
    "    'num_file_creations': 0, 'num_shells': 0, 'num_access_files': 0,\n",
    "    'num_outbound_cmds': 0, 'is_host_login': 0, 'is_guest_login': 0,\n",
    "    'count': 10, 'srv_count': 10, 'serror_rate': 0.0, 'srv_serror_rate': 0.0,\n",
    "    'rerror_rate': 0.0, 'srv_rerror_rate': 0.0, 'same_srv_rate': 1.0,\n",
    "    'diff_srv_rate': 0.0, 'srv_diff_host_rate': 0.0, 'dst_host_count': 255,\n",
    "    'dst_host_srv_count': 255, 'dst_host_same_srv_rate': 1.0,\n",
    "    'dst_host_diff_srv_rate': 0.0, 'dst_host_same_src_port_rate': 0.0,\n",
    "    'dst_host_srv_diff_host_rate': 0.0, 'dst_host_serror_rate': 0.0,\n",
    "    'dst_host_srv_rerror_rate': 0.0, 'dst_host_rerror_rate': 0.0,\n",
    "    'dst_host_srv_rerror_rate': 0.0, 'level': 0\n",
    "}\n",
    "\n",
    "result = predict_network_anomaly(single_record)\n",
    "print(f\"\\nSingle Record Prediction:\")\n",
    "print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f851ad34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
